{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db5272cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "from torchtext import datasets\n",
    "from torchtext import data\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.data import Example, Dataset\n",
    "\n",
    "import mlxtend, torchmetrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm.auto import tqdm\n",
    "from torchmetrics import ConfusionMatrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "import spacy\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c85c9f1",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89040e4",
   "metadata": {},
   "source": [
    "## BiLSTM with Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41583293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_1(nn.Module):\n",
    "    def __init__(self, emb_matrix, hidden_size):\n",
    "        super().__init__()\n",
    "        #embed the index corresponding to the number\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings=emb_matrix, freeze=True, padding_idx=1) #added padding_idx = 1 to not affect gradient\n",
    "\n",
    "        #feed batches into LSTM\n",
    "        self.forward_and_backward_LSTM = nn.LSTM(input_size=300, #embedding dimension\n",
    "                                    hidden_size=hidden_size, #hyperparameter \n",
    "                                    num_layers=2,\n",
    "                                    bidirectional=True,\n",
    "                                    batch_first=False,\n",
    "                                    dropout=.5\n",
    "                                    )\n",
    "        \n",
    "        self.attention_head = nn.MultiheadAttention(embed_dim=hidden_size*2, \n",
    "                                                    num_heads=1, \n",
    "                                                    batch_first=False)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "                                        # nn.Linear(in_features=hidden_size*2*3, out_features=hidden_size*2*3),\n",
    "                                        nn.Linear(in_features=hidden_size*2*4, out_features=hidden_size*2*4),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(),\n",
    "                                        # nn.Linear(in_features=hidden_size*2*3, out_features=1)\n",
    "                                        nn.Linear(in_features=hidden_size*2*4, out_features=1)\n",
    "                                        )\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input_indices, input_lengths):\n",
    "        word_embedding = self.embedding(input_indices)\n",
    "        #use pack_padded_sequence in order to make computation efficient\n",
    "        packed_embedding = nn.utils.rnn.pack_padded_sequence(enforce_sorted=False, input=word_embedding, lengths=input_lengths.cpu())\n",
    "\n",
    "        #pass through LSTM\n",
    "        packed_output, (hn, cn) = self.forward_and_backward_LSTM(packed_embedding) #(seq_len, batch, hidden_size*2)\n",
    "        \n",
    "        \n",
    "        #Max and mean pooling\n",
    "        unpacked_output, lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        attention_output, attention_weights = self.attention_head(unpacked_output, unpacked_output, unpacked_output)\n",
    "\n",
    "\n",
    "        h_final = torch.cat((hn[0], hn[1]), dim=1) # (batch_size, hidden_size*2)\n",
    "        mean_pool = torch.mean(unpacked_output, dim=0)\n",
    "        max_pool, _ = torch.max(unpacked_output, dim=0)\n",
    "        attention_pool = torch.mean(attention_output, dim=0)\n",
    "\n",
    "        #combine\n",
    "        features_pre_classification = torch.cat([h_final, mean_pool, max_pool, attention_pool], dim=1) #(batch_size, hidden_size*2*3)\n",
    "\n",
    "        res = self.classifier(features_pre_classification).squeeze(dim=1)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7abff40",
   "metadata": {},
   "source": [
    "## 4 Layer Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc8fd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_2_4(nn.Module):\n",
    "    def __init__(self, emb_matrix):\n",
    "        super().__init__()\n",
    "        #embed the index corresponding to the number\n",
    "        emb_dim = emb_matrix.size(1)\n",
    "        vocab_size = emb_matrix.size(0)\n",
    "\n",
    "        # self.embedding = BERTEmbeddings(emb_matrix).to(device)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings=emb_matrix, freeze=True, padding_idx=0) #added padding_idx = 0 to not affect gradient\n",
    "        \n",
    "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(\n",
    "                                                      d_model=768, #hidden size H in paper\n",
    "                                                      nhead =12,  # attention heads (768/12=64 per head)\n",
    "                                                      dim_feedforward=1536, #~4*H\n",
    "                                                      activation=\"gelu\",\n",
    "                                                      batch_first=True\n",
    "\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=4)\n",
    "\n",
    "        #classification head\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc = nn.Linear(768, 1)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input_indices, input_lengths):\n",
    "        word_embedding = self.embedding(input_indices)\n",
    "        hidden_states = self.transformer(word_embedding)\n",
    "\n",
    "        cls_embedding = hidden_states[:,0,:]\n",
    "\n",
    "        res = self.fc(self.dropout(cls_embedding)).squeeze(1)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed95e46",
   "metadata": {},
   "source": [
    "## 6 Layer Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8b50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_2_6(nn.Module):\n",
    "    def __init__(self, emb_matrix):\n",
    "        super().__init__()\n",
    "        #embed the index corresponding to the number\n",
    "        emb_dim = emb_matrix.size(1)\n",
    "        vocab_size = emb_matrix.size(0)\n",
    "\n",
    "        # self.embedding = BERTEmbeddings(emb_matrix).to(device)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings=emb_matrix, freeze=True, padding_idx=0) #added padding_idx = 0 to not affect gradient\n",
    "        \n",
    "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(\n",
    "                                                      d_model=768, #hidden size H in paper\n",
    "                                                      nhead =12,  # attention heads (768/12=64 per head)\n",
    "                                                      dim_feedforward=1536, #~4*H\n",
    "                                                      activation=\"gelu\",\n",
    "                                                      batch_first=True\n",
    "\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=6)\n",
    "\n",
    "        #classification head\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc = nn.Linear(768, 1)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input_indices, input_lengths):\n",
    "        word_embedding = self.embedding(input_indices)\n",
    "        hidden_states = self.transformer(word_embedding)\n",
    "\n",
    "        cls_embedding = hidden_states[:,0,:]\n",
    "\n",
    "        res = self.fc(self.dropout(cls_embedding)).squeeze(1)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5662f204",
   "metadata": {},
   "source": [
    "## 8 Layer Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac675cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_2_8(nn.Module):\n",
    "    def __init__(self, emb_matrix):\n",
    "        super().__init__()\n",
    "        #embed the index corresponding to the number\n",
    "        emb_dim = emb_matrix.size(1)\n",
    "        vocab_size = emb_matrix.size(0)\n",
    "\n",
    "        # self.embedding = BERTEmbeddings(emb_matrix).to(device)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings=emb_matrix, freeze=True, padding_idx=0) #added padding_idx = 0 to not affect gradient\n",
    "        \n",
    "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(\n",
    "                                                      d_model=768, #hidden size H in paper\n",
    "                                                      nhead =12,  # attention heads (768/12=64 per head)\n",
    "                                                      dim_feedforward=1536, #~4*H\n",
    "                                                      activation=\"gelu\",\n",
    "                                                      batch_first=True,\n",
    "                                                      norm_first=True\n",
    "\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=8)\n",
    "\n",
    "        #classification head\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc = nn.Linear(768, 1)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input_indices, input_lengths):\n",
    "        word_embedding = self.embedding(input_indices)\n",
    "        hidden_states = self.transformer(word_embedding)\n",
    "\n",
    "        cls_embedding = hidden_states[:,0,:]\n",
    "\n",
    "        res = self.fc(self.dropout(cls_embedding)).squeeze(1)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34dd327",
   "metadata": {},
   "source": [
    "## Combinining BiLSTM and Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5640e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_3(nn.Module):    \n",
    "    def __init__(self, emb_matrix, hidden_size=256):\n",
    "        super().__init__()\n",
    "        self.model1 = Model_1(emb_matrix, hidden_size)\n",
    "        self.model2 = Model_2_4(emb_matrix)\n",
    "\n",
    "        # Replace their final classifiers with \"feature extractors\"\n",
    "        self.model1.classifier = nn.Identity()\n",
    "        self.model2.fc = nn.Identity()\n",
    "\n",
    "        # Joint classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2*4 + 768, 512),  # concat Model1 feats + Model2 feats\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_indices, input_lengths):\n",
    "        feats1 = self.model1(input_indices, input_lengths)  # [batch, hidden*2*4]\n",
    "        feats2 = self.model2(input_indices, input_lengths)  # [batch, 768]\n",
    "\n",
    "        combined_feats = torch.cat([feats1, feats2], dim=1)\n",
    "        res = self.classifier(combined_feats).squeeze(1)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7a34a8",
   "metadata": {},
   "source": [
    "# Loading Models In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c7023d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (model_0-env)",
   "language": "python",
   "name": "model_0_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
