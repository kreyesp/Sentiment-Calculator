{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c22c17e",
   "metadata": {},
   "source": [
    "## Papers Read\n",
    "https://arxiv.org/pdf/1706.03762\n",
    "\n",
    "https://arxiv.org/pdf/1810.04805"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "746db4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "from torchtext import datasets\n",
    "from torchtext import data\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.data import Example, Dataset\n",
    "\n",
    "import mlxtend, torchmetrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm.auto import tqdm\n",
    "from torchmetrics import ConfusionMatrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565f08ba",
   "metadata": {},
   "source": [
    "# Getting data ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1e478",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46a2bb1",
   "metadata": {},
   "source": [
    "Dataset Being used is from https://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0179d55",
   "metadata": {},
   "source": [
    "This time I will be using WordPiece as my tokenizer, similar to the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d854e881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['i', 'absolutely', 'loved', 'this', 'movie', '!', 'it', 'was', 'amazing', '.']\n",
      "Token IDs: [1045, 7078, 3866, 2023, 3185, 999, 2009, 2001, 6429, 1012]\n",
      "tensor([[ 101, 1045, 7078,  ...,    0,    0,    0]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"I absolutely loved this movie! It was amazing.\"\n",
    "\n",
    "# Encode into WordPiece tokens\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Convert to IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# Or directly encode into IDs (with [CLS] and [SEP])\n",
    "encoded = tokenizer.encode_plus(\n",
    "    text,\n",
    "    add_special_tokens=True,   # adds [CLS] and [SEP]\n",
    "    max_length=2400,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\"        # returns PyTorch tensors\n",
    ")\n",
    "\n",
    "print(encoded[\"input_ids\"])   # token IDs (with CLS, SEP, padding)\n",
    "print(encoded[\"attention_mask\"])  # 1 = real token, 0 = padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2a98b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[CLS]', '[SEP]')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.cls_token, tokenizer.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "757215cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to create this function to tokenize since i am using an old torchtext version 0.6.0\n",
    "def tokenize_fn(text):\n",
    "    # return [tokenizer.cls_token] + tokenizer.tokenize(text) + [tokenizer.sep_token]\n",
    "    return  tokenizer.tokenize(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51dcd37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_field = data.Field(sequential=True,\n",
    "                                tokenize=tokenize_fn,\n",
    "                                lower=False,             \n",
    "                                include_lengths=True,\n",
    "                                use_vocab=True,\n",
    "                                batch_first=True,        # helpful later for BERT models\n",
    "                                pad_token=tokenizer.pad_token,\n",
    "                                unk_token=tokenizer.unk_token,\n",
    "                                init_token=tokenizer.cls_token,  # add [CLS] at start\n",
    "                                eos_token=tokenizer.sep_token    # add [SEP] at end\n",
    "                                )\n",
    "\n",
    "train_label_field = data.Field(sequential=False, use_vocab=True, unk_token=None)\n",
    "\n",
    "review_id_field = data.Field(sequential=False, use_vocab=False, unk_token=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f9a0959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDB dataset\n",
    "train_dataset, test_dataset = datasets.IMDB.splits(\n",
    "    text_field=train_text_field,\n",
    "    label_field=train_label_field\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset, validation_dataset = train_dataset.split(\n",
    "                                                        split_ratio=0.8,\n",
    "                                                        random_state=random.seed(43)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c9fd80",
   "metadata": {},
   "source": [
    "### Understanding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51dcc1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 5000, 25000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(validation_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "845bfde4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2313"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find the longest tokenization\n",
    "max_tokens_length = -1\n",
    "for example in train_dataset:\n",
    "    if len(example.text)>max_tokens_length:\n",
    "        max_tokens_length = len(example.text)\n",
    "\n",
    "max_tokens_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29f18147",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_example = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd1158e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_first_example = \"\"\n",
    "for token in first_example.text:\n",
    "    full_first_example+=token\n",
    "    full_first_example+= \" \"\n",
    "    if(token==\".\"):\n",
    "        full_first_example+='\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3437cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text tokens: ['midnight', 'cowboy', 'made', 'a', 'big', 'fuss', 'when', 'it', 'was', 'released', 'in', '1969', ',', 'drawing', 'an', 'x', 'rating', '.', 'by', 'today', \"'\", 's', 'standards', ',', 'it', 'would', 'be', 'hard', 'pressed', 'to', 'pull', 'an', 'r', 'rating', '.', 'jon', 'vo', '##ight', ',', 'who', 'has', 'been', 'better', ',', 'is', 'competent', 'in', 'his', 'role', 'as', 'joe', 'buck', ',', 'an', 'out', 'of', 'town', 'hi', '##ck', 'wanting', 'to', 'make', 'it', 'big', 'with', 'the', 'ladies', 'in', 'new', 'york', 'city', '.', 'he', 'meets', 'a', 'seed', '##y', 'street', 'hu', '##stle', '##r', 'named', 'rats', '##o', 'ri', '##zzo', ',', 'who', 'tries', 'to', 'be', '##fr', '##ien', '##d', 'buck', 'for', 'his', 'own', 'purposes', '.', 'the', 'two', 'eventually', 'forge', 'a', 'bond', 'that', 'is', 'both', 'touching', 'and', 'pathetic', '.', 'as', 'rats', '##o', ',', 'dustin', 'hoffman', 'simply', 'shine', '##s', '.', 'hoffman', 'has', 'often', 'been', 'brilliant', ',', 'but', 'never', 'more', 'so', 'than', 'in', 'this', 'portrayal', '.', 'he', 'is', 'so', 'into', 'character', 'that', 'all', 'else', 'around', 'him', 'pale', '##s', 'in', 'comparison', '.', 'losing', 'the', 'academy', 'award', 'to', 'john', 'wayne', 'is', 'one', 'of', 'the', 'most', 'ridiculous', 'decisions', 'ever', 'made', 'by', 'the', 'academy', 'of', 'motion', 'picture', 'arts', 'and', 'sciences', '.', 'director', 'sc', '##hl', '##ess', '##inger', 'has', 'a', 'def', '##t', 'hand', 'with', 'his', 'production', ',', 'but', 'this', 'film', 'has', 'a', 'gr', '##ung', '##y', 'under', '##bell', '##y', 'that', 'leaves', 'a', 'bad', 'taste', 'in', 'the', 'mouth', 'of', 'the', 'viewer', '.', 'worth', 'seeing', 'for', 'hoffman', \"'\", 's', 'performance', 'alone', '.']\n",
      "Full Review: midnight cowboy made a big fuss when it was released in 1969 , drawing an x rating . \n",
      "by today ' s standards , it would be hard pressed to pull an r rating . \n",
      "jon vo ##ight , who has been better , is competent in his role as joe buck , an out of town hi ##ck wanting to make it big with the ladies in new york city . \n",
      "he meets a seed ##y street hu ##stle ##r named rats ##o ri ##zzo , who tries to be ##fr ##ien ##d buck for his own purposes . \n",
      "the two eventually forge a bond that is both touching and pathetic . \n",
      "as rats ##o , dustin hoffman simply shine ##s . \n",
      "hoffman has often been brilliant , but never more so than in this portrayal . \n",
      "he is so into character that all else around him pale ##s in comparison . \n",
      "losing the academy award to john wayne is one of the most ridiculous decisions ever made by the academy of motion picture arts and sciences . \n",
      "director sc ##hl ##ess ##inger has a def ##t hand with his production , but this film has a gr ##ung ##y under ##bell ##y that leaves a bad taste in the mouth of the viewer . \n",
      "worth seeing for hoffman ' s performance alone . \n",
      "\n",
      "Label: pos\n"
     ]
    }
   ],
   "source": [
    "print(\"Text tokens:\", first_example.text)\n",
    "print(\"Full Review:\", full_first_example)\n",
    "print(\"Label:\", first_example.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096c7738",
   "metadata": {},
   "source": [
    "## BERT has inputs in specific form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941fa033",
   "metadata": {},
   "source": [
    "For example this sentence: \"The movie was amazing and thrilling\"\n",
    "\n",
    "Will be split like: [CLS] the movie was amaz ##ing and thrill ##ing . [SEP]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23d3bec",
   "metadata": {},
   "source": [
    "Then it will assign an embedding for each token by summing:\n",
    "\n",
    "    - Token vector embedding\n",
    "    - Segment embedding (what sentence belongs to)\n",
    "        - For my use I will just have each token belonging to the same sentence for ease of use\n",
    "    - Position embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d6f054",
   "metadata": {},
   "source": [
    "## Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f46aac38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3294467b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.dataset.Dataset at 0x22fecbbde80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44bd6fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_field.build_vocab(train_dataset, \n",
    "                             vectors=None,  # we’ll replace this with BERT vocab later if needed\n",
    "                             max_size=30000)\n",
    "\n",
    "train_label_field.build_vocab(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd605aed",
   "metadata": {},
   "source": [
    "Replace vocab with WordPiece vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee38630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create dummy counter since BERT vocab is prebuilt\n",
    "dummy_counter = Counter()\n",
    "\n",
    "# Initialize Vocab\n",
    "train_text_field.vocab = Vocab(dummy_counter, specials=[])\n",
    "\n",
    "# Load HuggingFace vocab\n",
    "bert_vocab = tokenizer.get_vocab()  # dict: token → id\n",
    "\n",
    "# Replace stoi and itos\n",
    "train_text_field.vocab.stoi = bert_vocab\n",
    "train_text_field.vocab.itos = {i: s for s, i in bert_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c930ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3866"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_field.vocab.stoi[\"loved\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7994fcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['i', 'loved', 'the', 'movie', ',', 'it', 'was', 'unbelievable', '!']\n",
      "Token IDs: [1045, 3866, 1996, 3185, 1010, 2009, 2001, 23653, 999]\n",
      "HuggingFace IDs: [1045, 3866, 1996, 3185, 1010, 2009, 2001, 23653, 999]\n"
     ]
    }
   ],
   "source": [
    "sample = \"I loved the movie, it was unbelievable!\"\n",
    "tokens = train_text_field.preprocess(sample)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "ids = [train_text_field.vocab.stoi[tok] for tok in tokens]\n",
    "print(\"Token IDs:\", ids)\n",
    "\n",
    "# Compare with HuggingFace tokenizer\n",
    "print(\"HuggingFace IDs:\", tokenizer.convert_tokens_to_ids(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79485759",
   "metadata": {},
   "source": [
    "### Want to Chunk dataset inorder to have sequences less thatn 512 tokens for efficient compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1121657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_dataset(original_dataset, max_len=512):\n",
    "    all_examples = []\n",
    "\n",
    "    for review_idx, example in enumerate(original_dataset):\n",
    "        tokens = example.text  # already tokenized\n",
    "\n",
    "        chunk_len = max_len - 2  # leave room for [CLS] and [SEP]\n",
    "        \n",
    "        # split into chunks\n",
    "        for i in range(0, len(tokens), chunk_len):\n",
    "            chunk_tokens = tokens[i:i + chunk_len]\n",
    "\n",
    "            # add special tokens for every chunk\n",
    "            # chunk_tokens = [original_dataset.fields['text'].init_token] + chunk_tokens + [original_dataset.fields['text'].eos_token]\n",
    "            chunk_tokens =  chunk_tokens \n",
    "\n",
    "            # create a new Example with review_id field\n",
    "            new_example = Example.fromlist(\n",
    "                [chunk_tokens, example.label, review_idx],\n",
    "                fields=[('text', original_dataset.fields['text']),\n",
    "                        ('label', original_dataset.fields['label']),\n",
    "                        ('review_id', review_id_field)]\n",
    "            )\n",
    "            all_examples.append(new_example)\n",
    "\n",
    "    # include the review_id field in the dataset\n",
    "    chunked_dataset = Dataset(all_examples, fields={\n",
    "        'text': original_dataset.fields['text'],\n",
    "        'label': original_dataset.fields['label'],\n",
    "        'review_id': review_id_field\n",
    "    })\n",
    "\n",
    "    return chunked_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e611420",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "chunked_train_dataset = chunk_dataset(train_dataset, max_len=512)\n",
    "chunked_val_dataset = chunk_dataset(validation_dataset, max_len=512)\n",
    "chunked_test_dataset = chunk_dataset(test_dataset, max_len=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f59561b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunked_train_dataset[3].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a05310ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = data.BucketIterator(\n",
    "    dataset=chunked_train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "validation_iter = data.BucketIterator(\n",
    "    dataset=chunked_val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "test_iter = data.BucketIterator(\n",
    "    dataset=chunked_test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6feddaf",
   "metadata": {},
   "source": [
    "### Sanity Check of chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bd8328e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total original reviews: 20000\n",
      "Total chunked examples: 23403\n",
      "Average chunks per review: 1.17\n",
      "Chunks for first 5 reviews: [1, 1, 1, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "review_chunks = defaultdict(int)\n",
    "for example in chunked_train_dataset:\n",
    "    rid = getattr(example, 'review_id', None)\n",
    "    if rid is not None:\n",
    "        review_chunks[rid] += 1\n",
    "\n",
    "# Inspect\n",
    "print(f\"Total original reviews: {len(train_dataset)}\")\n",
    "print(f\"Total chunked examples: {len(chunked_train_dataset)}\")\n",
    "print(f\"Average chunks per review: {sum(review_chunks.values())/len(review_chunks):.2f}\")\n",
    "print(f\"Chunks for first 5 reviews: {[review_chunks[i] for i in range(5)]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f998f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum chunks per review: 1\n",
      "Maximum chunks per review: 5\n",
      "Median chunks per review: 1\n",
      "Total chunks: 23403\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunk_counts = list(review_chunks.values())\n",
    "\n",
    "print(f\"Minimum chunks per review: {min(chunk_counts)}\")\n",
    "print(f\"Maximum chunks per review: {max(chunk_counts)}\")\n",
    "print(f\"Median chunks per review: {sorted(chunk_counts)[len(chunk_counts)//2]}\")\n",
    "print(f\"Total chunks: {sum(chunk_counts)}\")\n",
    "\n",
    "# Optional: plot histogram\n",
    "plt.hist(chunk_counts, bins=50)\n",
    "plt.xlabel(\"Chunks per review\")\n",
    "plt.ylabel(\"Number of reviews\")\n",
    "plt.title(\"Distribution of chunks per review\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d401680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Review ID 0 has 1 chunks\n",
      "  Chunk 0 length: 225 tokens\n",
      "\n",
      "Review ID 1 has 1 chunks\n",
      "  Chunk 0 length: 298 tokens\n",
      "\n",
      "Review ID 2 has 1 chunks\n",
      "  Chunk 0 length: 220 tokens\n",
      "\n",
      "Review ID 3 has 2 chunks\n",
      "  Chunk 0 length: 510 tokens\n",
      "  Chunk 1 length: 316 tokens\n",
      "\n",
      "Review ID 4 has 1 chunks\n",
      "  Chunk 0 length: 421 tokens\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    rid = i\n",
    "    chunks_for_review = [ex for ex in chunked_train_dataset if ex.review_id == rid]\n",
    "    print(f\"\\nReview ID {rid} has {len(chunks_for_review)} chunks\")\n",
    "    for j, chunk in enumerate(chunks_for_review):\n",
    "        print(f\"  Chunk {j} length: {len(chunk.text)} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f1334fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['memoirs',\n",
       " 'of',\n",
       " 'a',\n",
       " 'ge',\n",
       " '##isha',\n",
       " 'is',\n",
       " 'a',\n",
       " 'beautifully',\n",
       " 'filmed',\n",
       " 'movie',\n",
       " ',',\n",
       " 'there',\n",
       " 'is',\n",
       " 'no',\n",
       " 'doubt',\n",
       " 'about',\n",
       " 'that',\n",
       " '.',\n",
       " 'and',\n",
       " 'the',\n",
       " 'acting',\n",
       " 'is',\n",
       " 'generally',\n",
       " 'excellent',\n",
       " ',',\n",
       " 'at',\n",
       " 'least',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'how',\n",
       " 'it',\n",
       " 'portrays',\n",
       " 'the',\n",
       " 'characters',\n",
       " 'as',\n",
       " 'they',\n",
       " 'are',\n",
       " 'scripted',\n",
       " '.',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " 'however',\n",
       " ',',\n",
       " 'so',\n",
       " 'many',\n",
       " 'details',\n",
       " 'small',\n",
       " 'and',\n",
       " 'large',\n",
       " 'are',\n",
       " 'just',\n",
       " '_',\n",
       " 'wrong',\n",
       " '_',\n",
       " 'that',\n",
       " 'it',\n",
       " 'just',\n",
       " 'bother',\n",
       " '##s',\n",
       " 'me',\n",
       " 'too',\n",
       " 'much',\n",
       " 'to',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'enjoy',\n",
       " 'it',\n",
       " 'fully',\n",
       " '.',\n",
       " 'a',\n",
       " 'small',\n",
       " 'detail',\n",
       " 'that',\n",
       " 'ty',\n",
       " '##pi',\n",
       " '##fies',\n",
       " 'the',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'lack',\n",
       " 'of',\n",
       " 'sensitivity',\n",
       " 'of',\n",
       " 'sorts',\n",
       " 'is',\n",
       " 'one',\n",
       " 'scene',\n",
       " '(',\n",
       " 'no',\n",
       " 'this',\n",
       " 'does',\n",
       " 'not',\n",
       " 'spoil',\n",
       " 'anything',\n",
       " ')',\n",
       " 'where',\n",
       " 'ma',\n",
       " '##me',\n",
       " '##ha',\n",
       " 'rings',\n",
       " 'a',\n",
       " 'bell',\n",
       " 'that',\n",
       " 'hangs',\n",
       " 'at',\n",
       " 'the',\n",
       " 'door',\n",
       " 'of',\n",
       " 'the',\n",
       " 'house',\n",
       " 'where',\n",
       " 'say',\n",
       " '##uri',\n",
       " 'lives',\n",
       " ',',\n",
       " 'on',\n",
       " 'a',\n",
       " 'snowy',\n",
       " 'winter',\n",
       " 'day',\n",
       " '.',\n",
       " 'the',\n",
       " 'bell',\n",
       " 'she',\n",
       " \"'\",\n",
       " 's',\n",
       " 'ringing',\n",
       " 'is',\n",
       " 'a',\n",
       " 'fu',\n",
       " '##uri',\n",
       " '##n',\n",
       " ',',\n",
       " 'or',\n",
       " 'wind',\n",
       " 'chi',\n",
       " '##me',\n",
       " '-',\n",
       " 'that',\n",
       " 'is',\n",
       " 'only',\n",
       " 'left',\n",
       " 'hanging',\n",
       " 'outside',\n",
       " 'of',\n",
       " 'houses',\n",
       " 'in',\n",
       " 'japan',\n",
       " 'in',\n",
       " 'the',\n",
       " 'summer',\n",
       " '!',\n",
       " 'people',\n",
       " 'in',\n",
       " 'traditional',\n",
       " 'japanese',\n",
       " 'homes',\n",
       " 'didn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'have',\n",
       " 'doorbell',\n",
       " '##s',\n",
       " '-',\n",
       " 'they',\n",
       " 'just',\n",
       " 'opened',\n",
       " 'the',\n",
       " 'door',\n",
       " 'and',\n",
       " 'announced',\n",
       " 'themselves',\n",
       " '.',\n",
       " '(',\n",
       " 'you',\n",
       " 'may',\n",
       " 'think',\n",
       " 'this',\n",
       " 'is',\n",
       " 'such',\n",
       " 'a',\n",
       " 'trivial',\n",
       " 'detail',\n",
       " ',',\n",
       " 'but',\n",
       " 'i',\n",
       " 'would',\n",
       " 'e',\n",
       " '##qua',\n",
       " '##te',\n",
       " 'this',\n",
       " 'to',\n",
       " 'a',\n",
       " 'movie',\n",
       " 'made',\n",
       " 'about',\n",
       " 'america',\n",
       " 'where',\n",
       " 'a',\n",
       " 'christmas',\n",
       " 'wreath',\n",
       " 'is',\n",
       " 'hanging',\n",
       " 'on',\n",
       " 'the',\n",
       " 'door',\n",
       " 'in',\n",
       " 'july',\n",
       " 'and',\n",
       " 'no',\n",
       " 'one',\n",
       " 'thinks',\n",
       " 'anything',\n",
       " 'of',\n",
       " 'it',\n",
       " '.',\n",
       " ')',\n",
       " 'and',\n",
       " 'don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'even',\n",
       " 'get',\n",
       " 'me',\n",
       " 'started',\n",
       " 'on',\n",
       " 'the',\n",
       " 'totally',\n",
       " 'wrong',\n",
       " 'hairs',\n",
       " '##tyle',\n",
       " '##s',\n",
       " 'given',\n",
       " 'to',\n",
       " 'the',\n",
       " 'mai',\n",
       " '##ko',\n",
       " 'and',\n",
       " 'ge',\n",
       " '##isha',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'vaguely',\n",
       " 'pan',\n",
       " '-',\n",
       " 'asian',\n",
       " '/',\n",
       " 'chinese',\n",
       " '/',\n",
       " 'kung',\n",
       " '-',\n",
       " 'fu',\n",
       " '-',\n",
       " 'is',\n",
       " '##h',\n",
       " ',',\n",
       " 'and',\n",
       " 'nothing',\n",
       " 'like',\n",
       " 'real',\n",
       " 'thing',\n",
       " '.',\n",
       " 'i',\n",
       " 'think',\n",
       " 'this',\n",
       " 'rather',\n",
       " 'cavalier',\n",
       " 'attitude',\n",
       " 'towards',\n",
       " 'the',\n",
       " 'culture',\n",
       " 'they',\n",
       " 'are',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'portray',\n",
       " 'really',\n",
       " 'comes',\n",
       " 'out',\n",
       " 'in',\n",
       " 'the',\n",
       " 'attitudes',\n",
       " 'and',\n",
       " 'the',\n",
       " 'portrayal',\n",
       " '##s',\n",
       " 'of',\n",
       " 'people',\n",
       " 'and',\n",
       " 'situations',\n",
       " 'too',\n",
       " '.',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " 'so',\n",
       " ',',\n",
       " 'i',\n",
       " 'suppose',\n",
       " 'that',\n",
       " 'the',\n",
       " 'less',\n",
       " 'you',\n",
       " 'know',\n",
       " 'about',\n",
       " 'japanese',\n",
       " 'culture',\n",
       " 'and',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'the',\n",
       " 'ge',\n",
       " '##isha',\n",
       " 'and',\n",
       " 'mai',\n",
       " '##ko',\n",
       " 'in',\n",
       " 'kyoto',\n",
       " '(',\n",
       " 'which',\n",
       " 'is',\n",
       " 'what',\n",
       " '\"',\n",
       " 'mi',\n",
       " '##ya',\n",
       " '##ko',\n",
       " '\"',\n",
       " 'is',\n",
       " ')',\n",
       " ',',\n",
       " 'then',\n",
       " 'i',\n",
       " 'suppose',\n",
       " 'the',\n",
       " 'more',\n",
       " 'you',\n",
       " 'will',\n",
       " 'enjoy',\n",
       " 'this',\n",
       " '.',\n",
       " 'i',\n",
       " 'honestly',\n",
       " 'think',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'could',\n",
       " 'have',\n",
       " 'been',\n",
       " 'so',\n",
       " 'much',\n",
       " 'better',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'as',\n",
       " 'it',\n",
       " 'is',\n",
       " ',',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'just',\n",
       " 'another',\n",
       " 'hollywood',\n",
       " 'version',\n",
       " 'of',\n",
       " '\"',\n",
       " 'exotic',\n",
       " 'japan',\n",
       " '\"',\n",
       " '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_train_dataset[21].text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10876025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 missing special tokens\n",
      "Chunk 1 missing special tokens\n",
      "Chunk 2 missing special tokens\n",
      "Chunk 3 missing special tokens\n",
      "Chunk 4 missing special tokens\n",
      "Chunk 5 missing special tokens\n",
      "Chunk 6 missing special tokens\n",
      "Chunk 7 missing special tokens\n",
      "Chunk 8 missing special tokens\n",
      "Chunk 9 missing special tokens\n"
     ]
    }
   ],
   "source": [
    "cls_token = train_text_field.init_token\n",
    "sep_token = train_text_field.eos_token\n",
    "\n",
    "for i, example in enumerate(chunked_train_dataset[:10]):\n",
    "    if example.text[0] != cls_token or example.text[-1] != sep_token:\n",
    "        print(f\"Chunk {i} missing special tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ccbac1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 3 belongs to review_id: 3\n",
      "First 10 tokens: ['hmm', '##m', ',', 'a', 'sports', 'team', 'is', 'in', 'a', 'plane']\n",
      "Last 10 tokens: ['in', 'the', 'himalayas', ',', 'then', 'why', 'doesn', \"'\", 't', 'he']\n",
      "\n",
      "Chunk 4 belongs to review_id: 3\n",
      "First 10 tokens: ['know', 'enough', 'to', 'make', 'shelter', 'and', 'set', 'traps', 'right', 'from']\n",
      "Last 10 tokens: ['the', 'very', 'end', ',', 'however', ',', 'is', 'quote', 'lame', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check which review each chunk belongs to\n",
    "for i in [3, 4]:\n",
    "    ex = chunked_train_dataset[i]\n",
    "    rid = getattr(ex, 'review_id', None)\n",
    "    print(f\"Chunk {i} belongs to review_id: {rid}\")\n",
    "    print(f\"First 10 tokens: {ex.text[:10]}\")\n",
    "    print(f\"Last 10 tokens: {ex.text[-10:]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05df8e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 length: 510 tokens, first: ['hmm', '##m', ',', 'a', 'sports'], last: ['why', 'doesn', \"'\", 't', 'he']\n",
      "Chunk 1 length: 316 tokens, first: ['know', 'enough', 'to', 'make', 'shelter'], last: [',', 'is', 'quote', 'lame', '.']\n"
     ]
    }
   ],
   "source": [
    "rid_to_check = chunked_train_dataset[3].review_id\n",
    "chunks_for_review = [ex for ex in chunked_train_dataset if ex.review_id == rid_to_check]\n",
    "for j, ex in enumerate(chunks_for_review):\n",
    "    print(f\"Chunk {j} length: {len(ex.text)} tokens, first: {ex.text[:5]}, last: {ex.text[-5:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6efc2267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks missing review_id: 0\n"
     ]
    }
   ],
   "source": [
    "missing_ids = [ex for ex in chunked_train_dataset if getattr(ex, 'review_id', None) is None]\n",
    "print(f\"Chunks missing review_id: {len(missing_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7516ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 labels in chunks: {'pos'}\n",
      "Review 1 labels in chunks: {'pos'}\n",
      "Review 2 labels in chunks: {'neg'}\n",
      "Review 3 labels in chunks: {'neg'}\n",
      "Review 4 labels in chunks: {'pos'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    rid = i\n",
    "    labels = set(ex.label for ex in chunked_train_dataset if ex.review_id == rid)\n",
    "    print(f\"Review {rid} labels in chunks: {labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96092f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max chunk length: 510\n",
      "Min chunk length: 1\n",
      "Average chunk length: 265.85\n"
     ]
    }
   ],
   "source": [
    "chunk_lengths = [len(ex.text) for ex in chunked_train_dataset]\n",
    "print(f\"Max chunk length: {max(chunk_lengths)}\")\n",
    "print(f\"Min chunk length: {min(chunk_lengths)}\")\n",
    "print(f\"Average chunk length: {sum(chunk_lengths)/len(chunk_lengths):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9bf6daa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_iter: 732 batches of 32\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of train_iter: {len(train_iter)} batches of {BATCH_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76f92130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(732, 185, 908)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iter), len(validation_iter), len(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "669deea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.vocab.Vocab at 0x22fe8316c10>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_field.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1a64bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 32]\n",
       "\t[.text]:('[torch.cuda.LongTensor of size 32x254 (GPU 0)]', '[torch.cuda.LongTensor of size 32 (GPU 0)]')\n",
       "\t[.label]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n",
       "\t[.review_id]:[torch.cuda.LongTensor of size 32 (GPU 0)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's see what is in train_iter\n",
    "first_batch = next(iter(train_iter))\n",
    "first_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d241c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch_features, first_batch_lengths = first_batch.text\n",
    "first_batch_labels = first_batch.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8b71a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 254]), torch.Size([32]), torch.Size([32]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch_features.shape, first_batch_lengths.shape, first_batch_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9aec622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  1996,  3185,  2003, 10468,  1996,  2466,  1997,  1037,  2845,\n",
       "        19215,  1005,  1055,  2709,  2000,  2014,  2188,  2352,  2005,  1996,\n",
       "         6715,  1997,  1037,  2905,  1013,  2767,  1012,  2045,  2024,  1037,\n",
       "         3232,  1997,  2060,  3576,  2466,  3210,  2008,  2453,  2941,  2022,\n",
       "         2062,  5875,  2084,  1996,  2028,  2579,  1010,  2021,  2027,  2024,\n",
       "         2025,  3929, 10641,  1012,  1996,  4563,  1997,  1996,  3185,  2003,\n",
       "         1996,  6715,  1010,  5256,  1010,  1998,  2101,  6704,  2058,  1996,\n",
       "         2925,  1997,  1037,  2451,  1997, 13675, 21821,  2008,  2191, 14421,\n",
       "         1998,  5271,  2068,  2000,  4965, 21092,  2021,  2024,  2085,  4394,\n",
       "         1996,  3063,  2040,  2081,  2037, 14421,  3006,  3085,  1012,  4593,\n",
       "         1010,  1996,  3185,  2003, 16655, 23194,  2098,  1012,  1996, 19215,\n",
       "         1005,  1055,  4990,  2013,  1996,  2103,  2000,  1996,  2352,  2003,\n",
       "         2019,  4654, 26775, 14194, 15370,  2135, 10866,  3345,  4536,  1998,\n",
       "        12517,  2361,  2083,  1996,  8494,  1012,  2672,  2008,  1005,  1055,\n",
       "         4011,  2000, 17894,  2149,  2007,  1996, 10047,  3549, 17759,  1997,\n",
       "         1996,  2845,  5957,  1012,  1996,  2352,  2993,  1010,  2107,  2004,\n",
       "         2009,  2003,  1010,  2003,  9613,  2011,  1037,  8009,  1997, 24835,\n",
       "         1998,  2028,  3287,  1010,  1996, 13440,  1997,  1996,  2757,  2611,\n",
       "         1012,  5719,  1996, 10658,  2449,  2003, 18636,  2005,  3071,  2920,\n",
       "         1998,  2776,  3849,  5263,  1012,  2087,  1997,  1996,  2143,  2003,\n",
       "         2915,  2007,  1037,  2192,  1011,  2218,  4950,  2008,  2071, 19653,\n",
       "        19029,  1012,  2178,  3291,  2005,  2530,  7193,  2003,  2008,  4942,\n",
       "        27430,  2123,  1005,  1056,  2421,  1996,  2774,  1998, 20342,  7666,\n",
       "         1997,  1996, 13675, 21821,  1012,  2123,  1005,  1056,  2175,  2000,\n",
       "         2023,  3185,  4983,  2017,  1005,  2128, 19376,  1999,  2845,  1012,\n",
       "          102,     0,     0,     0], device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch_features[21,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a587ebf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(26)\n",
    "random_index = torch.randint(0, 32, size=[1]).item()\n",
    "print(random_index)\n",
    "\n",
    "review, label = first_batch_features[random_index,:], first_batch_labels[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "502359b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  1996,  3185,  2003, 10468,  1996,  2466,  1997,  1037,  2845,\n",
       "        19215,  1005,  1055,  2709,  2000,  2014,  2188,  2352,  2005,  1996,\n",
       "         6715,  1997,  1037,  2905,  1013,  2767,  1012,  2045,  2024,  1037,\n",
       "         3232,  1997,  2060,  3576,  2466,  3210,  2008,  2453,  2941,  2022,\n",
       "         2062,  5875,  2084,  1996,  2028,  2579,  1010,  2021,  2027,  2024,\n",
       "         2025,  3929, 10641,  1012,  1996,  4563,  1997,  1996,  3185,  2003,\n",
       "         1996,  6715,  1010,  5256,  1010,  1998,  2101,  6704,  2058,  1996,\n",
       "         2925,  1997,  1037,  2451,  1997, 13675, 21821,  2008,  2191, 14421,\n",
       "         1998,  5271,  2068,  2000,  4965, 21092,  2021,  2024,  2085,  4394,\n",
       "         1996,  3063,  2040,  2081,  2037, 14421,  3006,  3085,  1012,  4593,\n",
       "         1010,  1996,  3185,  2003, 16655, 23194,  2098,  1012,  1996, 19215,\n",
       "         1005,  1055,  4990,  2013,  1996,  2103,  2000,  1996,  2352,  2003,\n",
       "         2019,  4654, 26775, 14194, 15370,  2135, 10866,  3345,  4536,  1998,\n",
       "        12517,  2361,  2083,  1996,  8494,  1012,  2672,  2008,  1005,  1055,\n",
       "         4011,  2000, 17894,  2149,  2007,  1996, 10047,  3549, 17759,  1997,\n",
       "         1996,  2845,  5957,  1012,  1996,  2352,  2993,  1010,  2107,  2004,\n",
       "         2009,  2003,  1010,  2003,  9613,  2011,  1037,  8009,  1997, 24835,\n",
       "         1998,  2028,  3287,  1010,  1996, 13440,  1997,  1996,  2757,  2611,\n",
       "         1012,  5719,  1996, 10658,  2449,  2003, 18636,  2005,  3071,  2920,\n",
       "         1998,  2776,  3849,  5263,  1012,  2087,  1997,  1996,  2143,  2003,\n",
       "         2915,  2007,  1037,  2192,  1011,  2218,  4950,  2008,  2071, 19653,\n",
       "        19029,  1012,  2178,  3291,  2005,  2530,  7193,  2003,  2008,  4942,\n",
       "        27430,  2123,  1005,  1056,  2421,  1996,  2774,  1998, 20342,  7666,\n",
       "         1997,  1996, 13675, 21821,  1012,  2123,  1005,  1056,  2175,  2000,\n",
       "         2023,  3185,  4983,  2017,  1005,  2128, 19376,  1999,  2845,  1012,\n",
       "          102,     0,     0,     0], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c594a4d",
   "metadata": {},
   "source": [
    "Each word is assigned an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "acb69f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] the movie is basically the story of a russian prostitute ' s return to her home village for the funeral of a sister / friend . there are a couple of other minor story lines that might actually be more interesting than the one taken , but they are not fully explored . the core of the movie is the funeral , wake , and later controversy over the future of a community of cr ##ones that make dolls and sell them to buy vodka but are now missing the artist who made their dolls market ##able . apparently , the movie is une ##dit ##ed . the prostitute ' s journey from the city to the village is an ex ##cr ##uc ##iating ##ly endless train ride and tram ##p through the mud . maybe that ' s supposed to impress us with the im ##men ##sity of the russian landscape . the village itself , such as it is , is inhabited by a legion of widows and one male , the consort of the dead girl . continuing the doll business is problematic for everyone involved and eventually seems impossible . most of the film is shot with a hand - held camera that could induce nausea . another problem for western viewers is that sub ##titles don ' t include the songs and lame ##nts of the cr ##ones . don ' t go to this movie unless you ' re fluent in russian . [SEP] [PAD] [PAD] [PAD] \n",
      "tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "string = \"\"\n",
    "for word in [train_text_field.vocab.itos[index.item()] for index in review]:\n",
    "    # print(index)\n",
    "    string += word + \" \"\n",
    "\n",
    "print(string)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b013011d",
   "metadata": {},
   "source": [
    "## Mapping vocab to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98c0a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89294a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30521"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train_text_field.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "36be1dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_embedding_matrix = bert.embeddings.word_embeddings.weight.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7570095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30522, 768])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13389c09",
   "metadata": {},
   "source": [
    "Now, each index in train_text_field.vocab.itos is mapped to a length 300 vector embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2cd1258b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD]'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_field.vocab.itos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3d141caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0446,  0.0061, -0.0022,  0.0023, -0.0365]),\n",
       " tensor([-0.0446,  0.0061, -0.0022,  0.0023, -0.0365], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_embedding_matrix[1996][:5], bert.embeddings.word_embeddings.weight[1996][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b40aaf",
   "metadata": {},
   "source": [
    "The embedding matrix has each word mapped to a vector now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4a1723",
   "metadata": {},
   "source": [
    "## Now need to take into account the token, positional, and segment embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "16d84869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_embeddings(max_len, hidden_size, device=device):\n",
    "    # Create position indices [0, 1, ..., max_len-1]\n",
    "    position = torch.arange(max_len, dtype=torch.float, device=device).unsqueeze(1)\n",
    "    # Create dimension indices [0, 1, ..., hidden_size-1]\n",
    "    div_term = torch.exp(torch.arange(0, hidden_size, 2, dtype=torch.float, device=device) *\n",
    "                         -(math.log(10000.0) / hidden_size))\n",
    "    \n",
    "    pe = torch.zeros(max_len, hidden_size, device=device)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)   # apply sin to even indices\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)   # apply cos to odd indices\n",
    "    \n",
    "    return pe  # shape: [max_len, hidden_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3f26b816",
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_embedding_matrix = sinusoidal_embeddings(max_len=2400, hidden_size=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc0be73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2400, 768])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c0fb470e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 254)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, seq_len = first_batch_features.size()\n",
    "_, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a011b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 254, 768])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch_pos_embeds = positional_embedding_matrix[:seq_len, :].unsqueeze(0).expand(BATCH_SIZE, -1, -1)\n",
    "first_batch_pos_embeds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6469d4a",
   "metadata": {},
   "source": [
    "## Creating the Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6f2f81e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_len: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        # Precompute sinusoidal matrix [max_len, hidden_dim]\n",
    "        pe = torch.zeros(max_len, hidden_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / hidden_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # [1, max_len, hidden_dim]\n",
    "\n",
    "    def forward(self, seq_len: int, batch_size: int):\n",
    "        # Return only the slice needed\n",
    "        return self.pe[:, :seq_len, :].expand(batch_size, -1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "70dd3067",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbeddings(nn.Module):\n",
    "    def __init__(self, emb_matrix: torch.Tensor, max_len: int = 512):\n",
    "        super().__init__()\n",
    "        vocab_size, hidden_dim = emb_matrix.shape\n",
    "        \n",
    "        # Token embeddings (frozen BERT pretrained embeddings you built earlier)\n",
    "        self.token_embeddings = nn.Embedding.from_pretrained(\n",
    "            emb_matrix, freeze=True, padding_idx=0\n",
    "        )\n",
    "\n",
    "        # Segment embeddings (2 segments: sentence A = 0, sentence B = 1, but you only need 1)\n",
    "        self.segment_embeddings = nn.Embedding(2, hidden_dim)\n",
    "\n",
    "        # Positional embeddings (sinusoidal, not trainable)\n",
    "        self.positional_embeddings = PositionalEmbedding(max_len, hidden_dim)\n",
    "\n",
    "    def forward(self, input_ids, segment_ids=None):\n",
    "        \"\"\"\n",
    "        input_ids: [batch_size, seq_len]\n",
    "        segment_ids: [batch_size, seq_len] (default 0s if not provided)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        \n",
    "        if segment_ids is None:\n",
    "            segment_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        token_embeds = self.token_embeddings(input_ids)                    # [B, L, H]\n",
    "        segment_embeds = self.segment_embeddings(segment_ids)              # [B, L, H]\n",
    "        pos_embeds = self.positional_embeddings(seq_len, batch_size)       # [B, L, H]\n",
    "\n",
    "        return token_embeds + segment_embeds + pos_embeds                  # [B, L, H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d3456b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTEmbeddings(\n",
       "  (token_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "  (segment_embeddings): Embedding(2, 768)\n",
       "  (positional_embeddings): PositionalEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing out making sure the embeddings work before plugging into transformer model\n",
    "embedding_model = BERTEmbeddings(vector_embedding_matrix).to(device)\n",
    "embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7b1ea07d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 254, 768])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_model(first_batch_features)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32877757",
   "metadata": {},
   "source": [
    "Having to decide to truncate my reviews to around 512 because i want to be able to process everything efficiently realistically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2369cf99",
   "metadata": {},
   "source": [
    "Will try to use chunking because I don't want to lose information from any review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bf12fd",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2c1c0c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "  correct = torch.eq(y_true, y_pred).sum().item() #torch.eq gets how many values are equal, them sum, them get only the item in the tensor\n",
    "  acc = (correct/len(y_pred))*100\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d681e1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               data_iter: torchtext.data.BucketIterator,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               train_loss_per_epoch,\n",
    "               device: torch.device = device):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    for batch in data_iter:\n",
    "        batch_features, batch_lengths = batch.text\n",
    "        batch_labels = batch.label\n",
    "\n",
    "        batch_features = batch_features.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        pred_labels = model(batch_features, batch_lengths)\n",
    "\n",
    "        # compute loss & accuracy per chunk\n",
    "        loss = loss_fn(pred_labels, batch_labels.float())\n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy_fn(y_true=batch_labels.float(),\n",
    "                                 y_pred=torch.round(torch.sigmoid(pred_labels)))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # average over batches\n",
    "    train_loss /= len(data_iter)\n",
    "    train_acc /= len(data_iter)\n",
    "\n",
    "    train_loss_per_epoch.append(train_loss)\n",
    "    print(f\"Train loss: {train_loss:.4f} | Train acc: {train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f796837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_step_review_metrics(model: torch.nn.Module,\n",
    "                              data_iter: torchtext.data.BucketIterator,\n",
    "                              loss_fn: torch.nn.Module,\n",
    "                              accuracy_fn,\n",
    "                              valid_loss_per_epoch,\n",
    "                              valid_acc_per_epoch,\n",
    "                              device: torch.device = device):\n",
    "    \n",
    "    model.eval()\n",
    "    valid_loss, valid_acc = 0, 0\n",
    "\n",
    "    review_preds = defaultdict(list)\n",
    "    review_labels = {}\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch in data_iter:\n",
    "            batch_features, batch_lengths = batch.text\n",
    "            batch_labels = batch.label\n",
    "            batch_review_ids = batch.review_id  # tensor of size [batch_size]\n",
    "\n",
    "            batch_features = batch_features.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            pred_logits = model(batch_features, batch_lengths)\n",
    "            pred_probs = torch.sigmoid(pred_logits)\n",
    "\n",
    "            # --- Chunk-level metrics ---\n",
    "            valid_loss += loss_fn(pred_logits, batch_labels.float()).item()\n",
    "            valid_acc += accuracy_fn(y_true=batch_labels.float(),\n",
    "                                     y_pred=torch.round(pred_probs))\n",
    "\n",
    "            # --- Collect predictions per review ---\n",
    "            for i, rid in enumerate(batch_review_ids):\n",
    "                review_preds[int(rid.item())].append(pred_probs[i].item())\n",
    "                review_labels[int(rid.item())] = int(batch_labels[i].item())\n",
    "\n",
    "    # Average chunk predictions per review\n",
    "    y_true, y_pred, y_prob = [], [], []\n",
    "    for rid in sorted(review_preds.keys()):\n",
    "        avg_prob = sum(review_preds[rid]) / len(review_preds[rid])\n",
    "        y_prob.append(avg_prob)\n",
    "        y_pred.append(1 if avg_prob >= 0.5 else 0)\n",
    "        y_true.append(review_labels[rid])\n",
    "\n",
    "    review_acc = sum([yp == yt for yp, yt in zip(y_pred, y_true)]) / len(y_true)\n",
    "\n",
    "    # Average chunk-level metrics\n",
    "    valid_loss /= len(data_iter)\n",
    "    valid_acc /= len(data_iter)\n",
    "\n",
    "    valid_loss_per_epoch.append(valid_loss)\n",
    "    valid_acc_per_epoch.append(valid_acc)\n",
    "\n",
    "    print(f\"Chunk-level loss: {valid_loss:.4f} | Chunk-level acc: {valid_acc:.4f}\")\n",
    "    print(f\"Review-level acc: {review_acc:.4f}\\n-----\")\n",
    "\n",
    "    return valid_loss, review_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5e060fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_mode_chunked(model, data_iter, device=device):\n",
    "    \"\"\"\n",
    "    Inference for models trained on chunked data. Aggregates predictions per review.\n",
    "    Works with torchtext 0.6.0 and chunked datasets.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    review_preds = defaultdict(list)  # store probs per review\n",
    "    review_labels = {}                # store true label per review\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm(data_iter, desc=\"Inference on chunks\"):\n",
    "            batch_features, batch_lengths = batch.text  # features and lengths\n",
    "            batch_labels = batch.label\n",
    "            batch_features = batch_features.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            logits = model(batch_features, batch_lengths)\n",
    "            probs = torch.sigmoid(logits).cpu()  # shape: [batch_size]\n",
    "\n",
    "            # map each chunk to its review_id\n",
    "            batch_review_ids = batch.review_id.cpu()  # shape: [batch_size]\n",
    "            for i in range(batch_features.size(0)):\n",
    "                rid = batch_review_ids[i].item()\n",
    "                review_preds[rid].append(probs[i].item())\n",
    "                review_labels[rid] = batch_labels[i].item()\n",
    "\n",
    "    # Aggregate per review\n",
    "    y_true, y_pred, y_prob = [], [], []\n",
    "    for rid in sorted(review_preds.keys()):\n",
    "        chunk_probs = review_preds[rid]\n",
    "        avg_prob = sum(chunk_probs) / len(chunk_probs)\n",
    "        y_prob.append(avg_prob)\n",
    "        y_pred.append(1 if avg_prob >= 0.5 else 0)\n",
    "        y_true.append(review_labels[rid])\n",
    "\n",
    "    if len(y_true) == 0:\n",
    "        print(\"⚠️ Warning: No review predictions found. Check that review_ids are set in dataset.\")\n",
    "        return None, None, None\n",
    "\n",
    "    y_true_tensor = torch.tensor(y_true)\n",
    "    y_pred_tensor = torch.tensor(y_pred)\n",
    "    y_prob_tensor = torch.tensor(y_prob)\n",
    "\n",
    "    correct = (y_pred_tensor == y_true_tensor).sum().item()\n",
    "    total = len(y_true_tensor)\n",
    "    print(f\"Correct predictions: {correct}/{total} ({correct/total:.2%})\")\n",
    "\n",
    "    return y_pred_tensor, y_true_tensor, y_prob_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "03e78691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function 1: Epoch-based curves ---\n",
    "def plot_epoch_metrics(epoch_list, train_loss_per_epoch, valid_loss_per_epoch, valid_acc_per_epoch):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Loss curves\n",
    "    axes[0].plot(epoch_list, [x.item() for x in train_loss_per_epoch], label=\"Train Loss\")\n",
    "    axes[0].plot(epoch_list, [x.item() for x in valid_loss_per_epoch], label=\"Validation Loss\")\n",
    "    axes[0].set_title(\"Train/Validation Loss per Epoch\")\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Accuracy curve\n",
    "    axes[1].plot(epoch_list, valid_acc_per_epoch, label=\"Validation Accuracy\", color=\"green\")\n",
    "    axes[1].set_title(\"Validation Accuracy per Epoch\")\n",
    "    axes[1].set_xlabel(\"Epoch\")\n",
    "    axes[1].set_ylabel(\"Accuracy\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- Function 2: Inference-based metrics ---\n",
    "def plot_inference_metrics(y_true_tensor, y_pred_tensor, y_prob_tensor, class_names):\n",
    "    # --- Confusion Matrix ---\n",
    "    confmat = ConfusionMatrix(num_classes=2, task=\"binary\")\n",
    "    confmat_tensor = confmat(preds=y_pred_tensor, target=y_true_tensor)\n",
    "\n",
    "    fig, ax = plot_confusion_matrix(\n",
    "        conf_mat=confmat_tensor.numpy(),\n",
    "        class_names=class_names,\n",
    "        figsize=(6,5)\n",
    "    )\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # --- ROC + Precision-Recall in one figure ---\n",
    "    y_probs_np = y_prob_tensor.numpy()\n",
    "    y_true_np = y_true_tensor.numpy()\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true_np, y_probs_np)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_true_np, y_probs_np)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12,5))\n",
    "\n",
    "    # ROC curve\n",
    "    axes[0].plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "    axes[0].plot([0, 1], [0, 1], \"k--\")\n",
    "    axes[0].set_xlabel(\"False Positive Rate\")\n",
    "    axes[0].set_ylabel(\"True Positive Rate\")\n",
    "    axes[0].set_title(\"ROC Curve\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Precision-Recall curve\n",
    "    axes[1].plot(recall, precision, label=\"PR curve\")\n",
    "    axes[1].set_xlabel(\"Recall\")\n",
    "    axes[1].set_ylabel(\"Precision\")\n",
    "    axes[1].set_title(\"Precision-Recall Curve\")\n",
    "    axes[1].legend(loc=\"lower left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5d39f3",
   "metadata": {},
   "source": [
    "# Model_2 (Transformer) Try to replicate bert structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42f2c92a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgc\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Clear cache\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mtorch\u001b[49m.cuda.empty_cache()\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Collect garbage\u001b[39;00m\n\u001b[32m      7\u001b[39m gc.collect()\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Clear cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Collect garbage\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2c993f",
   "metadata": {},
   "source": [
    "## 4 layer transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "857822e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_2_4(nn.Module):\n",
    "    def __init__(self, emb_matrix):\n",
    "        super().__init__()\n",
    "        #embed the index corresponding to the number\n",
    "        emb_dim = emb_matrix.size(1)\n",
    "        vocab_size = emb_matrix.size(0)\n",
    "\n",
    "        # self.embedding = BERTEmbeddings(emb_matrix).to(device)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings=emb_matrix, freeze=True, padding_idx=0) #added padding_idx = 0 to not affect gradient\n",
    "        \n",
    "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(\n",
    "                                                      d_model=768, #hidden size H in paper\n",
    "                                                      nhead =12,  # attention heads (768/12=64 per head)\n",
    "                                                      dim_feedforward=1536, #~4*H\n",
    "                                                      activation=\"gelu\",\n",
    "                                                      batch_first=True\n",
    "\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=4)\n",
    "\n",
    "        #classification head\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc = nn.Linear(768, 1)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input_indices, input_lengths):\n",
    "        word_embedding = self.embedding(input_indices)\n",
    "        hidden_states = self.transformer(word_embedding)\n",
    "\n",
    "        cls_embedding = hidden_states[:,0,:]\n",
    "\n",
    "        res = self.fc(self.dropout(cls_embedding)).squeeze(1)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b72f074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Model_2_4(vector_embedding_matrix).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4df2bf95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 254])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "495a1479",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model_2(first_batch_features.to(device), first_batch_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d70f493d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1f7aca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn_2 = nn.BCEWithLogitsLoss()\n",
    "optimizer_2 = torch.optim.AdamW(params=model_2.parameters(),\n",
    "                            lr=1e-4,\n",
    "                            weight_decay=1e-5)\n",
    "\n",
    "\n",
    "train_loss_per_epoch_2 = []\n",
    "valid_loss_per_epoch_2 = []\n",
    "valid_acc_per_epoch_2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c20feed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847998f680b64399884a7da54cb0bc26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Train loss: 0.4832 | Train acc: 76.0261\n",
      "Chunk-level loss: 0.3843 | Chunk-level acc: 82.4662\n",
      "Review-level acc: 0.8448\n",
      "-----\n",
      "✅ New best model saved to 'Best_Transformer_8_Layer.pth'!\n",
      "Epoch 2/20\n",
      "Train loss: 0.3567 | Train acc: 84.5248\n",
      "Chunk-level loss: 0.3545 | Chunk-level acc: 84.4088\n",
      "Review-level acc: 0.8634\n",
      "-----\n",
      "✅ New best model saved to 'Best_Transformer_8_Layer.pth'!\n",
      "Epoch 3/20\n",
      "Train loss: 0.3135 | Train acc: 86.5018\n",
      "Chunk-level loss: 0.3556 | Chunk-level acc: 84.6622\n",
      "Review-level acc: 0.8612\n",
      "-----\n",
      "⚠️ No improvement. Patience: 1/2\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[142]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# 🔹 Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_fn_2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccuracy_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loss_per_epoch_2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# 🔹 Validate\u001b[39;00m\n\u001b[32m     26\u001b[39m val_loss, review_acc = valid_step_review_metrics(\n\u001b[32m     27\u001b[39m     model_2,\n\u001b[32m     28\u001b[39m     validation_iter,\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     valid_acc_per_epoch_2,\n\u001b[32m     33\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mtrain_step\u001b[39m\u001b[34m(model, data_iter, loss_fn, optimizer, accuracy_fn, train_loss_per_epoch, device)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# compute loss & accuracy per chunk\u001b[39;00m\n\u001b[32m     23\u001b[39m loss = loss_fn(pred_labels, batch_labels.float())\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m train_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m train_acc += accuracy_fn(y_true=batch_labels.float(),\n\u001b[32m     26\u001b[39m                          y_pred=torch.round(torch.sigmoid(pred_labels)))\n\u001b[32m     28\u001b[39m optimizer.zero_grad()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "epochs = 20\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 2\n",
    "patience_counter = 0\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_2, mode=\"min\", factor=0.5, patience=1\n",
    ")\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    # 🔹 Train\n",
    "    train_step(\n",
    "        model_2,\n",
    "        train_iter,\n",
    "        loss_fn_2,\n",
    "        optimizer_2,\n",
    "        accuracy_fn,\n",
    "        train_loss_per_epoch_2,\n",
    "    )\n",
    "\n",
    "    # 🔹 Validate\n",
    "    val_loss, review_acc = valid_step_review_metrics(\n",
    "        model_2,\n",
    "        validation_iter,\n",
    "        loss_fn_2,\n",
    "        accuracy_fn,\n",
    "        valid_loss_per_epoch_2,\n",
    "        valid_acc_per_epoch_2,\n",
    "    )\n",
    "\n",
    "    # 🔹 Scheduler step\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # 🔹 Early stopping + save checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model_2.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer_2.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"train_loss_per_epoch\": train_loss_per_epoch_2,\n",
    "            \"valid_loss_per_epoch\": valid_loss_per_epoch_2,\n",
    "            \"valid_acc_per_epoch\": valid_acc_per_epoch_2,\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, \"Best_Transformer_4_Layer.pth\")  # ✅ save checkpoint\n",
    "\n",
    "        print(\"✅ New best model saved to 'Best_Transformer_4_Layer.pth'!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"⚠️ No improvement. Patience: {patience_counter}/{patience}\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"⏹️ Early stopping triggered.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "40bb0e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model restored from epoch 2, best val loss: 0.3545\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"Best_Transformer_4_Layer.pth\", map_location=device)\n",
    "\n",
    "model_2.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer_2.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "\n",
    "train_loss_per_epoch_2 = checkpoint[\"train_loss_per_epoch\"]\n",
    "valid_loss_per_epoch_2 = checkpoint[\"valid_loss_per_epoch\"]\n",
    "valid_acc_per_epoch_2 = checkpoint[\"valid_acc_per_epoch\"]\n",
    "\n",
    "best_val_loss = checkpoint[\"best_val_loss\"]\n",
    "start_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "print(f\"✅ Model restored from epoch {checkpoint['epoch']+1}, best val loss: {best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a85d44e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model_2_4(\n",
       "  (embedding): Embedding(30522, 768, padding_idx=0)\n",
       "  (transformer_encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=768, out_features=1536, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=1536, out_features=768, bias=True)\n",
       "    (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=1536, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "87f2b7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced06f6744bf452a888f2138b28b9537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference on chunks:   0%|          | 0/908 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 21620/25000 (86.48%)\n"
     ]
    }
   ],
   "source": [
    "y_true_tensor_2, y_pred_tensor_2, y_prob_tensor_2 = inference_mode_chunked(model_2,test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b6eac8",
   "metadata": {},
   "source": [
    "## Failed 8 layer transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f270bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_2(nn.Module):\n",
    "    def __init__(self, emb_matrix):\n",
    "        super().__init__()\n",
    "        #embed the index corresponding to the number\n",
    "        emb_dim = emb_matrix.size(1)\n",
    "        vocab_size = emb_matrix.size(0)\n",
    "\n",
    "        # self.embedding = BERTEmbeddings(emb_matrix).to(device)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings=emb_matrix, freeze=True, padding_idx=0) #added padding_idx = 0 to not affect gradient\n",
    "        \n",
    "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(\n",
    "                                                      d_model=768, #hidden size H in paper\n",
    "                                                      nhead =12,  # attention heads (768/12=64 per head)\n",
    "                                                      dim_feedforward=1536, #~4*H\n",
    "                                                      activation=\"gelu\",\n",
    "                                                      batch_first=True\n",
    "\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=8)\n",
    "\n",
    "        #classification head\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc = nn.Linear(768, 1)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input_indices, input_lengths):\n",
    "        word_embedding = self.embedding(input_indices)\n",
    "        hidden_states = self.transformer(word_embedding)\n",
    "\n",
    "        cls_embedding = hidden_states[:,0,:]\n",
    "\n",
    "        res = self.fc(self.dropout(cls_embedding)).squeeze(1)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a7e9ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Model_2(vector_embedding_matrix).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c4d7a4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 254])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fcf9ee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model_2(first_batch_features.to(device), first_batch_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9263f1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ea363cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn_2 = nn.BCEWithLogitsLoss()\n",
    "optimizer_2 = torch.optim.AdamW(params=model_2.parameters(),\n",
    "                            lr=1e-4,\n",
    "                            weight_decay=1e-5)\n",
    "\n",
    "\n",
    "train_loss_per_epoch_2 = []\n",
    "valid_loss_per_epoch_2 = []\n",
    "valid_acc_per_epoch_2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "de25c471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e414a7bfe44f1f95995497379e821f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Train loss: 0.7135 | Train acc: 49.7885\n",
      "Chunk-level loss: 0.6966 | Chunk-level acc: 49.4707\n",
      "Review-level acc: 0.5000\n",
      "-----\n",
      "✅ New best model saved to 'Best_Transformer_8_Layer.pth'!\n",
      "Epoch 2/20\n",
      "Train loss: 0.6983 | Train acc: 50.3990\n",
      "Chunk-level loss: 0.6937 | Chunk-level acc: 50.5293\n",
      "Review-level acc: 0.5000\n",
      "-----\n",
      "✅ New best model saved to 'Best_Transformer_8_Layer.pth'!\n",
      "Epoch 3/20\n",
      "Train loss: 0.6985 | Train acc: 50.4292\n",
      "Chunk-level loss: 0.6960 | Chunk-level acc: 50.5293\n",
      "Review-level acc: 0.5000\n",
      "-----\n",
      "⚠️ No improvement. Patience: 1/2\n",
      "Epoch 4/20\n",
      "Train loss: 0.6972 | Train acc: 49.9507\n",
      "Chunk-level loss: 0.6934 | Chunk-level acc: 49.4707\n",
      "Review-level acc: 0.5000\n",
      "-----\n",
      "✅ New best model saved to 'Best_Transformer_8_Layer.pth'!\n",
      "Epoch 5/20\n",
      "Train loss: 0.6960 | Train acc: 49.6814\n",
      "Chunk-level loss: 0.6947 | Chunk-level acc: 50.2027\n",
      "Review-level acc: 0.5000\n",
      "-----\n",
      "⚠️ No improvement. Patience: 1/2\n",
      "Epoch 6/20\n",
      "Train loss: 0.6958 | Train acc: 49.8999\n",
      "Chunk-level loss: 0.6945 | Chunk-level acc: 50.5293\n",
      "Review-level acc: 0.5000\n",
      "-----\n",
      "⚠️ No improvement. Patience: 2/2\n",
      "⏹️ Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "epochs = 20\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 2\n",
    "patience_counter = 0\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_2, mode=\"min\", factor=0.5, patience=1\n",
    ")\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    # 🔹 Train\n",
    "    train_step(\n",
    "        model_2,\n",
    "        train_iter,\n",
    "        loss_fn_2,\n",
    "        optimizer_2,\n",
    "        accuracy_fn,\n",
    "        train_loss_per_epoch_2,\n",
    "    )\n",
    "\n",
    "    # 🔹 Validate\n",
    "    val_loss, review_acc = valid_step_review_metrics(\n",
    "        model_2,\n",
    "        validation_iter,\n",
    "        loss_fn_2,\n",
    "        accuracy_fn,\n",
    "        valid_loss_per_epoch_2,\n",
    "        valid_acc_per_epoch_2,\n",
    "    )\n",
    "\n",
    "    # 🔹 Scheduler step\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # 🔹 Early stopping + save checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model_2.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer_2.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"train_loss_per_epoch\": train_loss_per_epoch_2,\n",
    "            \"valid_loss_per_epoch\": valid_loss_per_epoch_2,\n",
    "            \"valid_acc_per_epoch\": valid_acc_per_epoch_2,\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, \"Best_Transformer_8_Layer.pth\")  # ✅ save checkpoint\n",
    "\n",
    "        print(\"✅ New best model saved to 'Best_Transformer_8_Layer.pth'!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"⚠️ No improvement. Patience: {patience_counter}/{patience}\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"⏹️ Early stopping triggered.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8914964",
   "metadata": {},
   "source": [
    "noted that the transformer was not training with 8 layers. this could be due to the lr and no pre-norm encoder. Will add in the pre norm. also lower lr to 3e-5. will also add in torch.nn.utils.clip_grad_norm_(model_2.parameters(), max_norm=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1ff336fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model restored from epoch 5, best val loss: 0.3727\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"Best_Transformer_6_Layer.pth\", map_location=device)\n",
    "\n",
    "model_2.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer_2.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "\n",
    "train_loss_per_epoch_2 = checkpoint[\"train_loss_per_epoch\"]\n",
    "valid_loss_per_epoch_2 = checkpoint[\"valid_loss_per_epoch\"]\n",
    "valid_acc_per_epoch_2 = checkpoint[\"valid_acc_per_epoch\"]\n",
    "\n",
    "best_val_loss = checkpoint[\"best_val_loss\"]\n",
    "start_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "print(f\"✅ Model restored from epoch {checkpoint['epoch']+1}, best val loss: {best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e29efac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d4bf703a1b477f9308f805acf71b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference on chunks:   0%|          | 0/908 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 21708/25000 (86.83%)\n"
     ]
    }
   ],
   "source": [
    "y_true_tensor_2, y_pred_tensor_2, y_prob_tensor_2 = inference_mode_chunked(model_2,test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51be67a1",
   "metadata": {},
   "source": [
    "## Trying to fix 8 layer transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b0eb8c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_2(nn.Module):\n",
    "    def __init__(self, emb_matrix):\n",
    "        super().__init__()\n",
    "        #embed the index corresponding to the number\n",
    "        emb_dim = emb_matrix.size(1)\n",
    "        vocab_size = emb_matrix.size(0)\n",
    "\n",
    "        # self.embedding = BERTEmbeddings(emb_matrix).to(device)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings=emb_matrix, freeze=True, padding_idx=0) #added padding_idx = 0 to not affect gradient\n",
    "        \n",
    "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(\n",
    "                                                      d_model=768, #hidden size H in paper\n",
    "                                                      nhead =12,  # attention heads (768/12=64 per head)\n",
    "                                                      dim_feedforward=1536, #~4*H\n",
    "                                                      activation=\"gelu\",\n",
    "                                                      batch_first=True,\n",
    "                                                      norm_first=True\n",
    "\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=8)\n",
    "\n",
    "        #classification head\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc = nn.Linear(768, 1)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input_indices, input_lengths):\n",
    "        word_embedding = self.embedding(input_indices)\n",
    "        hidden_states = self.transformer(word_embedding)\n",
    "\n",
    "        cls_embedding = hidden_states[:,0,:]\n",
    "\n",
    "        res = self.fc(self.dropout(cls_embedding)).squeeze(1)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "28e3c5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Model_2(vector_embedding_matrix).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2cbe0e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 254])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "54e18460",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model_2(first_batch_features.to(device), first_batch_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5aaebced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8b152cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn_2 = nn.BCEWithLogitsLoss()\n",
    "optimizer_2 = torch.optim.AdamW(model_2.parameters(), lr=3e-5, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "train_loss_per_epoch_2 = []\n",
    "valid_loss_per_epoch_2 = []\n",
    "valid_acc_per_epoch_2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9a1aa44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_8_layer(model: torch.nn.Module,\n",
    "               data_iter: torchtext.data.BucketIterator,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               train_loss_per_epoch,\n",
    "               device: torch.device = device):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    for batch in data_iter:\n",
    "        batch_features, batch_lengths = batch.text\n",
    "        batch_labels = batch.label\n",
    "\n",
    "        batch_features = batch_features.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        pred_labels = model(batch_features, batch_lengths)\n",
    "\n",
    "        # compute loss & accuracy per chunk\n",
    "        loss = loss_fn(pred_labels, batch_labels.float())\n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy_fn(y_true=batch_labels.float(),\n",
    "                                 y_pred=torch.round(torch.sigmoid(pred_labels)))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    # average over batches\n",
    "    train_loss /= len(data_iter)\n",
    "    train_acc /= len(data_iter)\n",
    "\n",
    "    train_loss_per_epoch.append(train_loss)\n",
    "    print(f\"Train loss: {train_loss:.4f} | Train acc: {train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4323d381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d019e7db6d614a8ca5ed4bd68c039d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Train loss: 0.6220 | Train acc: 74.8890\n",
      "Chunk-level loss: 0.3983 | Chunk-level acc: 84.3243\n",
      "Review-level acc: 0.8606\n",
      "-----\n",
      "✅ New best model saved to 'Best_Transformer_8_Layer_Revised.pth'!\n",
      "Epoch 2/20\n",
      "Train loss: 0.4195 | Train acc: 83.3504\n",
      "Chunk-level loss: 0.4124 | Chunk-level acc: 85.2421\n",
      "Review-level acc: 0.8704\n",
      "-----\n",
      "⚠️ No improvement. Patience: 1/2\n",
      "Epoch 3/20\n",
      "Train loss: 0.3639 | Train acc: 85.8230\n",
      "Chunk-level loss: 0.4268 | Chunk-level acc: 83.4122\n",
      "Review-level acc: 0.8508\n",
      "-----\n",
      "⚠️ No improvement. Patience: 2/2\n",
      "⏹️ Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "epochs = 20\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 2\n",
    "patience_counter = 0\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_2, mode=\"min\", factor=0.5, patience=1\n",
    ")\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    # 🔹 Train\n",
    "    train_step_8_layer(\n",
    "        model_2,\n",
    "        train_iter,\n",
    "        loss_fn_2,\n",
    "        optimizer_2,\n",
    "        accuracy_fn,\n",
    "        train_loss_per_epoch_2,\n",
    "    )\n",
    "\n",
    "    # 🔹 Validate\n",
    "    val_loss, review_acc = valid_step_review_metrics(\n",
    "        model_2,\n",
    "        validation_iter,\n",
    "        loss_fn_2,\n",
    "        accuracy_fn,\n",
    "        valid_loss_per_epoch_2,\n",
    "        valid_acc_per_epoch_2,\n",
    "    )\n",
    "\n",
    "    # 🔹 Scheduler step\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # 🔹 Early stopping + save checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model_2.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer_2.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"train_loss_per_epoch\": train_loss_per_epoch_2,\n",
    "            \"valid_loss_per_epoch\": valid_loss_per_epoch_2,\n",
    "            \"valid_acc_per_epoch\": valid_acc_per_epoch_2,\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, \"Best_Transformer_8_Layer_Revised.pth\")  # ✅ save checkpoint\n",
    "\n",
    "        print(\"✅ New best model saved to 'Best_Transformer_8_Layer_Revised.pth'!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"⚠️ No improvement. Patience: {patience_counter}/{patience}\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"⏹️ Early stopping triggered.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "85a72ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model restored from epoch 1, best val loss: 0.3983\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"Best_Transformer_8_Layer_Revised.pth\", map_location=device)\n",
    "\n",
    "model_2.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer_2.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "\n",
    "train_loss_per_epoch_2 = checkpoint[\"train_loss_per_epoch\"]\n",
    "valid_loss_per_epoch_2 = checkpoint[\"valid_loss_per_epoch\"]\n",
    "valid_acc_per_epoch_2 = checkpoint[\"valid_acc_per_epoch\"]\n",
    "\n",
    "best_val_loss = checkpoint[\"best_val_loss\"]\n",
    "start_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "print(f\"✅ Model restored from epoch {checkpoint['epoch']+1}, best val loss: {best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "dcdd8838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ba33930b494c8295551bb48962d70b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference on chunks:   0%|          | 0/908 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 21602/25000 (86.41%)\n"
     ]
    }
   ],
   "source": [
    "y_true_tensor_2, y_pred_tensor_2, y_prob_tensor_2 = inference_mode_chunked(model_2,test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9e3584",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Model_2(vector_embedding_matrix).to(device)\n",
    "\n",
    "loss_fn_2 = nn.BCEWithLogitsLoss()\n",
    "optimizer_2 = torch.optim.AdamW(params=model_2.parameters(),\n",
    "                                lr=1e-4,\n",
    "                                weight_decay=1e-5)\n",
    "\n",
    "train_loss_per_epoch_2 = []\n",
    "valid_loss_per_epoch_2 = []\n",
    "valid_acc_per_epoch_2 = []\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "epochs = 20\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 2\n",
    "patience_counter = 0\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_2, mode=\"min\", factor=0.5, patience=1\n",
    ")\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    train_step_8_layer(model_2, train_iter, loss_fn_2, optimizer_2, accuracy_fn, train_loss_per_epoch_2)\n",
    "    \n",
    "    val_loss, review_acc = valid_step_review_metrics(\n",
    "        model_2,\n",
    "        validation_iter,\n",
    "        loss_fn_2,\n",
    "        accuracy_fn,\n",
    "        valid_loss_per_epoch_2,\n",
    "        valid_acc_per_epoch_2,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Scheduler step uses chunk-level loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping on chunk-level loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = {k: v.clone() for k, v in model_2.state_dict().items()}\n",
    "        print(\"✅ New best model saved (in memory)!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"⚠️ No improvement. Patience: {patience_counter}/{patience}\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"⏹️ Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "\n",
    "# 🔹 Load best model\n",
    "model_2.load_state_dict(best_model_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9de63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_chunked_batch = next(iter(train_iter))\n",
    "first_chunked_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c7cd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_tensor_2, y_pred_tensor_2, y_prob_tensor_2 = inference_mode_chunked(model_2,test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1918ee8c",
   "metadata": {},
   "source": [
    "# Testing BILSTM with an Attention Head on Chunked Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f8b34e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_1(nn.Module):\n",
    "    def __init__(self, emb_matrix, hidden_size):\n",
    "        super().__init__()\n",
    "        #embed the index corresponding to the number\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings=emb_matrix, freeze=True, padding_idx=0) #added padding_idx = 1 to not affect gradient\n",
    "\n",
    "        #feed batches into LSTM\n",
    "        self.forward_and_backward_LSTM = nn.LSTM(input_size=768, #embedding dimension\n",
    "                                    hidden_size=hidden_size, #hyperparameter \n",
    "                                    num_layers=2,\n",
    "                                    bidirectional=True,\n",
    "                                    batch_first=True,\n",
    "                                    dropout=.5\n",
    "                                    )\n",
    "        \n",
    "        self.attention_head = nn.MultiheadAttention(embed_dim=hidden_size*2, \n",
    "                                                    num_heads=1, \n",
    "                                                    batch_first=True)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "                                        # nn.Linear(in_features=hidden_size*2*3, out_features=hidden_size*2*3),\n",
    "                                        nn.Linear(in_features=hidden_size*2*4, out_features=hidden_size*2*4),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(),\n",
    "                                        # nn.Linear(in_features=hidden_size*2*3, out_features=1)\n",
    "                                        nn.Linear(in_features=hidden_size*2*4, out_features=1)\n",
    "                                        )\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input_indices, input_lengths):\n",
    "        word_embedding = self.embedding(input_indices)\n",
    "        #use pack_padded_sequence in order to make computation efficient\n",
    "        packed_embedding = nn.utils.rnn.pack_padded_sequence(enforce_sorted=False, input=word_embedding, lengths=input_lengths.cpu(),\n",
    "                                                             batch_first=True)\n",
    "\n",
    "        #pass through LSTM\n",
    "        packed_output, (hn, cn) = self.forward_and_backward_LSTM(packed_embedding) #(seq_len, batch, hidden_size*2)\n",
    "        \n",
    "        \n",
    "        #Max and mean pooling\n",
    "        unpacked_output, lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        attention_output, attention_weights = self.attention_head(unpacked_output, unpacked_output, unpacked_output)\n",
    "\n",
    "\n",
    "        h_final = torch.cat((hn[0], hn[1]), dim=1) # (batch_size, hidden_size*2)\n",
    "        mean_pool = torch.mean(unpacked_output, dim=1)\n",
    "        max_pool, _ = torch.max(unpacked_output, dim=1)\n",
    "        attention_pool = torch.mean(attention_output, dim=1)\n",
    "\n",
    "        #combine\n",
    "        features_pre_classification = torch.cat([h_final, mean_pool, max_pool, attention_pool], dim=1) #(batch_size, hidden_size*2*3)\n",
    "\n",
    "        res = self.classifier(features_pre_classification).squeeze(dim=1)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "34f3a89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30522, 768])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1bafb8c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model_1(\n",
       "  (embedding): Embedding(30522, 768, padding_idx=0)\n",
       "  (forward_and_backward_LSTM): LSTM(768, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (attention_head): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=2048, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = Model_1(vector_embedding_matrix, hidden_size=256).to(device)\n",
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "984dfe0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 254])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3116b1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model_1(first_batch_features, first_batch_lengths)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "be1b95f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn_1 = nn.BCEWithLogitsLoss()\n",
    "optimizer_1 = torch.optim.AdamW(params=model_1.parameters(),\n",
    "                            lr=1e-4,\n",
    "                            weight_decay=1e-5)\n",
    "\n",
    "\n",
    "train_loss_per_epoch_1 = []\n",
    "valid_loss_per_epoch_1 = []\n",
    "valid_acc_per_epoch_1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d8ed05bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b33d60a49ec9483288a2cedb324c32d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Train loss: 0.6422 | Train acc: 58.9706\n",
      "Chunk-level loss: 0.4683 | Chunk-level acc: 78.2489\n",
      "Review-level acc: 0.7964\n",
      "-----\n",
      "✅ New best model saved to 'BILSTM_Chunked.pth'!\n",
      "Epoch 2/20\n",
      "Train loss: 0.4570 | Train acc: 78.7056\n",
      "Chunk-level loss: 0.4199 | Chunk-level acc: 81.3288\n",
      "Review-level acc: 0.8284\n",
      "-----\n",
      "✅ New best model saved to 'BILSTM_Chunked.pth'!\n",
      "Epoch 3/20\n",
      "Train loss: 0.4382 | Train acc: 79.5691\n",
      "Chunk-level loss: 0.4017 | Chunk-level acc: 82.5676\n",
      "Review-level acc: 0.8386\n",
      "-----\n",
      "✅ New best model saved to 'BILSTM_Chunked.pth'!\n",
      "Epoch 4/20\n",
      "Train loss: 0.4089 | Train acc: 81.2849\n",
      "Chunk-level loss: 0.4150 | Chunk-level acc: 81.8300\n",
      "Review-level acc: 0.8334\n",
      "-----\n",
      "⚠️ No improvement. Patience: 1/2\n",
      "Epoch 5/20\n",
      "Train loss: 0.3657 | Train acc: 83.7319\n",
      "Chunk-level loss: 0.4028 | Chunk-level acc: 81.3851\n",
      "Review-level acc: 0.8282\n",
      "-----\n",
      "⚠️ No improvement. Patience: 2/2\n",
      "⏹️ Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "epochs = 20\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 2\n",
    "patience_counter = 0\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_1, mode=\"min\", factor=0.5, patience=1\n",
    ")\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    # 🔹 Train\n",
    "    train_step(\n",
    "        model_1,\n",
    "        train_iter,\n",
    "        loss_fn_1,\n",
    "        optimizer_1,\n",
    "        accuracy_fn,\n",
    "        train_loss_per_epoch_1,\n",
    "    )\n",
    "\n",
    "    # 🔹 Validate\n",
    "    val_loss, review_acc = valid_step_review_metrics(\n",
    "        model_1,\n",
    "        validation_iter,\n",
    "        loss_fn_1,\n",
    "        accuracy_fn,\n",
    "        valid_loss_per_epoch_1,\n",
    "        valid_acc_per_epoch_1,\n",
    "    )\n",
    "\n",
    "    # 🔹 Scheduler step\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # 🔹 Early stopping + save checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model_1.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer_1.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"train_loss_per_epoch\": train_loss_per_epoch_1,\n",
    "            \"valid_loss_per_epoch\": valid_loss_per_epoch_1,\n",
    "            \"valid_acc_per_epoch\": valid_acc_per_epoch_1,\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, \"BILSTM_Chunked.pth\")  # ✅ save checkpoint\n",
    "\n",
    "        print(\"✅ New best model saved to 'BILSTM_Chunked.pth'!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"⚠️ No improvement. Patience: {patience_counter}/{patience}\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"⏹️ Early stopping triggered.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6144c745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model restored from epoch 3, best val loss: 0.4017\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"BILSTM_Chunked.pth\", map_location=device)\n",
    "\n",
    "model_1.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer_1.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "\n",
    "train_loss_per_epoch_1 = checkpoint[\"train_loss_per_epoch\"]\n",
    "valid_loss_per_epoch_1 = checkpoint[\"valid_loss_per_epoch\"]\n",
    "valid_acc_per_epoch_1 = checkpoint[\"valid_acc_per_epoch\"]\n",
    "\n",
    "best_val_loss = checkpoint[\"best_val_loss\"]\n",
    "start_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "print(f\"✅ Model restored from epoch {checkpoint['epoch']+1}, best val loss: {best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "88a197d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21bf7d7ccc774dbfb7c331f079ec6359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference on chunks:   0%|          | 0/908 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 20775/25000 (83.10%)\n"
     ]
    }
   ],
   "source": [
    "y_true_tensor_1, y_pred_tensor_1, y_prob_tensor_1 = inference_mode_chunked(model_1,test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44335fa8",
   "metadata": {},
   "source": [
    "# Model_3 (Combining Both BILSTM and Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "48072231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_3(nn.Module):    \n",
    "    def __init__(self, emb_matrix, hidden_size=256):\n",
    "        super().__init__()\n",
    "        self.model1 = Model_1(emb_matrix, hidden_size)\n",
    "        self.model2 = Model_2_4(emb_matrix)\n",
    "\n",
    "        # Replace their final classifiers with \"feature extractors\"\n",
    "        self.model1.classifier = nn.Identity()\n",
    "        self.model2.fc = nn.Identity()\n",
    "\n",
    "        # Joint classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2*4 + 768, 512),  # concat Model1 feats + Model2 feats\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_indices, input_lengths):\n",
    "        feats1 = self.model1(input_indices, input_lengths)  # [batch, hidden*2*4]\n",
    "        feats2 = self.model2(input_indices, input_lengths)  # [batch, 768]\n",
    "\n",
    "        combined_feats = torch.cat([feats1, feats2], dim=1)\n",
    "        res = self.classifier(combined_feats).squeeze(1)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "950b7ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model_3(\n",
       "  (model1): Model_1(\n",
       "    (embedding): Embedding(30522, 768, padding_idx=0)\n",
       "    (forward_and_backward_LSTM): LSTM(768, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "    (attention_head): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (classifier): Identity()\n",
       "  )\n",
       "  (model2): Model_2_4(\n",
       "    (embedding): Embedding(30522, 768, padding_idx=0)\n",
       "    (transformer_encoder_layer): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=768, out_features=1536, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=1536, out_features=768, bias=True)\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=1536, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1536, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=2816, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3 = Model_3(vector_embedding_matrix, hidden_size=256).to(device)\n",
    "model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2d919621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.4606e-01, -2.6902e-02,  1.5643e-01, -7.6610e-02,  1.7158e-01,\n",
       "        -6.1204e-02, -3.3359e-01, -1.0791e-02, -5.0379e-01, -7.3869e-02,\n",
       "        -1.3574e-01,  2.1698e-01,  1.0392e-01, -3.3645e-01,  1.6944e-01,\n",
       "         2.1322e-01, -7.7716e-02,  3.2431e-04, -2.7284e-02, -1.9802e-01,\n",
       "        -1.2082e-01, -7.0134e-02, -1.7881e-01, -3.2972e-01, -5.7940e-01,\n",
       "        -5.6145e-02,  1.2746e-01, -1.1012e-01,  3.4782e-01, -1.6016e-01,\n",
       "         1.0644e-01, -1.4626e-01], device='cuda:0', grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model_3(first_batch_features, first_batch_lengths)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3b9431e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn_3 = nn.BCEWithLogitsLoss()\n",
    "optimizer_3 = torch.optim.AdamW(params=model_3.parameters(),\n",
    "                            lr=1e-4,\n",
    "                            weight_decay=1e-5)\n",
    "\n",
    "\n",
    "train_loss_per_epoch_3 = []\n",
    "valid_loss_per_epoch_3 = []\n",
    "valid_acc_per_epoch_3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d60a6945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea7c5330237473fa119eed442950426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Train loss: 0.4756 | Train acc: 75.4746\n",
      "Chunk-level loss: 0.3933 | Chunk-level acc: 81.9932\n",
      "Review-level acc: 0.8380\n",
      "-----\n",
      "✅ New best model saved to 'BILSTM_And_Transformer.pth'!\n",
      "Epoch 2/20\n",
      "Train loss: 0.3601 | Train acc: 84.2388\n",
      "Chunk-level loss: 0.3576 | Chunk-level acc: 84.5664\n",
      "Review-level acc: 0.8612\n",
      "-----\n",
      "✅ New best model saved to 'BILSTM_And_Transformer.pth'!\n",
      "Epoch 3/20\n",
      "Train loss: 0.3186 | Train acc: 86.4762\n",
      "Chunk-level loss: 0.3328 | Chunk-level acc: 85.1520\n",
      "Review-level acc: 0.8702\n",
      "-----\n",
      "✅ New best model saved to 'BILSTM_And_Transformer.pth'!\n",
      "Epoch 4/20\n",
      "Train loss: 0.2873 | Train acc: 88.0515\n",
      "Chunk-level loss: 0.3394 | Chunk-level acc: 85.5912\n",
      "Review-level acc: 0.8684\n",
      "-----\n",
      "⚠️ No improvement. Patience: 1/2\n",
      "Epoch 5/20\n",
      "Train loss: 0.2598 | Train acc: 89.2767\n",
      "Chunk-level loss: 0.3612 | Chunk-level acc: 86.2162\n",
      "Review-level acc: 0.8752\n",
      "-----\n",
      "⚠️ No improvement. Patience: 2/2\n",
      "⏹️ Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "epochs = 20\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 2\n",
    "patience_counter = 0\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_3, mode=\"min\", factor=0.5, patience=1\n",
    ")\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    # 🔹 Train\n",
    "    train_step(\n",
    "        model_3,\n",
    "        train_iter,\n",
    "        loss_fn_3,\n",
    "        optimizer_3,\n",
    "        accuracy_fn,\n",
    "        train_loss_per_epoch_3,\n",
    "    )\n",
    "\n",
    "    # 🔹 Validate\n",
    "    val_loss, review_acc = valid_step_review_metrics(\n",
    "        model_3,\n",
    "        validation_iter,\n",
    "        loss_fn_3,\n",
    "        accuracy_fn,\n",
    "        valid_loss_per_epoch_3,\n",
    "        valid_acc_per_epoch_3,\n",
    "    )\n",
    "\n",
    "    # 🔹 Scheduler step\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # 🔹 Early stopping + save checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model_3.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer_3.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"train_loss_per_epoch\": train_loss_per_epoch_3,\n",
    "            \"valid_loss_per_epoch\": valid_loss_per_epoch_3,\n",
    "            \"valid_acc_per_epoch\": valid_acc_per_epoch_3,\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, \"BILSTM_And_Transformer.pth\")  # ✅ save checkpoint\n",
    "\n",
    "        print(\"✅ New best model saved to 'BILSTM_And_Transformer.pth'!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"⚠️ No improvement. Patience: {patience_counter}/{patience}\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"⏹️ Early stopping triggered.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "65b1d619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model restored from epoch 3, best val loss: 0.3328\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"BILSTM_And_Transformer.pth\", map_location=device)\n",
    "\n",
    "model_3.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer_3.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "\n",
    "train_loss_per_epoch_3 = checkpoint[\"train_loss_per_epoch\"]\n",
    "valid_loss_per_epoch_3 = checkpoint[\"valid_loss_per_epoch\"]\n",
    "valid_acc_per_epoch_3 = checkpoint[\"valid_acc_per_epoch\"]\n",
    "\n",
    "best_val_loss = checkpoint[\"best_val_loss\"]\n",
    "start_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "print(f\"✅ Model restored from epoch {checkpoint['epoch']+1}, best val loss: {best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6e99e4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24bcd187c4a04055aa36fcb45a57bbd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference on chunks:   0%|          | 0/908 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 21924/25000 (87.70%)\n"
     ]
    }
   ],
   "source": [
    "y_true_tensor_3, y_pred_tensor_3, y_prob_tensor_3 = inference_mode_chunked(model_3,test_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (model_0-env)",
   "language": "python",
   "name": "model_0_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
